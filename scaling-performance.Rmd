# Performance {#performance}

```{r, include = FALSE}
source("common.R")
options(tibble.print_min = 6, tibble.print_max = 6)
```

Shiny can support thousands or tens of thousands of users, if developed correctly.
But most Shiny apps were quickly thrown together to solve a pressing analytic need, and often start with pretty terrible performance.
In some ways, this is a feature of Shiny --- it allows you to quickly prototype a proof of concept that works for you, before figuring out how to make it fast so that it can be used by a bunch of people simultaneously.
But if you don't realise that app is likely because you've optimised for human iteration speed), making a Shiny app available to many people will be frustrating.

Fortunately, however, Shiny comes with a bunch of tools to help improve performance.
In this chapter you'll learn:

-   How to use shinyloadtest to simulate the use of your app by many people.
    This allows you to figure out if you have a problem, and to measure the impact of your performance improvement.

-   To use profvis to identify performance bottlenecks.
    It is extremely difficult to develop a good intuition for what parts of your app are likely to be slow.
    Fortunately there's no need because we can measure and visualise.

-   A grab bag of useful techniques to improve performance, particularly focussing on techniques that allow you to take advantage of multiple users.

-   We'll finish up with a little applied psychology to give a few techniques that can help your app *feel* as fast as possible.

For a demo of the whole process, I recommend Joe Cheng's rstudio::conf(2019) keynote [Shiny in production: principles, best practices, and tools](https://rstudio.com/resources/rstudioconf-2019/shiny-in-production-principles-practices-and-tools/){.uri} where he works through the whole process with a realistic app.
This is also written in the [scaling an app case study](https://rstudio.github.io/shinyloadtest/articles/case-study-scaling.html) on the shinyloadtest website.

But to get started, we'll build up your mental model of Shiny performance with a restaurant metaphor.
We'll use this metaphor throughout the rest of the chapter.

```{r setup}
library(shiny)
```

Particularly thanks go to my RStudio colleagues Joe Cheng, Sean Lopp, and Alan Dipert, whose RStudio::conf() talks were particularly helpful when writing this chapter.

## Dining at restaurant Shiny

When considering performance, it's useful to think of a Shiny app as a restaurant[^scaling-performance-1].
Each customer (user) comes into the restaurant (the server) and makes an order (a request), which is then prepared by a chef (the R process).
This metaphor is useful because like a restaurant, one R process can serve multiple users at the same time, and there similar ways to dealing with increased demand.

[^scaling-performance-1]: Thanks to Sean Lopp for this analogy from his rstudio::conf(2018) talk [Scaling Shiny to 10,000 users](https://rstudio.com/resources/rstudioconf-2018/scaling-shiny/){.uri}.
    I highly recommend watching it if you have any doubt that Shiny apps can handle thousands of users.

To begin, you might investigate ways to make your current chef more efficient (optimise your R code).
To do so, you'd first spend some time watching to find the bottlenecks in their method (profiling) and then brainstorming ways to make it faster (optimising).
For example, maybe you hire a prep cook who can come in before the first customer and chop some vegetables (prepares the data), or you could invest in a time-saving gadget (a faster R package).

Or you might think about adding more chefs (processes) to the kitchen (server).
Fortunately, it's much easier[^scaling-performance-2] to add more processes than hire trained chefs.
If you keep hiring more chefs, eventually the kitchen will get too full and you'll need to add more equipment.
This is called scaling **up**[^scaling-performance-3], adding more resources (memory or cores) to an existing server to allow it to run more processes
.

[^scaling-performance-2]: Again, this depends on exactly how your app is deployed, but it's typically a prominently labelled setting.

[^scaling-performance-3]: Or vertical scaling

At some point, you'll have crammed as many chefs into your restaurant as you possibly can and its still not enough.
At that point, you'll need to building more restaurants.
This is called scaling **out**[^scaling-performance-4], and means using multiple servers.
This allows you to scale to any number of customers, but there's no way to serve one customer from multiple restaurants, so you need some way to direct customers to the least busy restaurant (a load balancer).
I won't talk more about scaling out in this chapter, because while the details are straightforward, they depend entirely on your deployment infrastructure.
It's good to know this option exists, and allows you app to scale to any number of users.

[^scaling-performance-4]: Or horizontal scaling

There's one major place where the metaphor breaks down: a normal chef can make multiple dishes at the same time, carefully interweaving the steps to do as much as possible in parallel.
Unfortunately, R can't do multiple things at the same because it's single threaded, i.e. each process can only do one thing at a time.
This is fine if all of the meals are fast to cook, but if someone requests 24-hour sous vide pork belly, everyone else has to wait until it's done.
Fortunately, you can work around this limitation using async programming.
Unfortunately, that's a complex topic and beyond the scope of this book, but you can learn more at <https://rstudio.github.io/promises/>.

## Benchmark

You almost always start by developing an app for yourself.
You have a personal chef who only ever has to serve one customer at a time (you!).
While you might be happy with performance, you might worry that your chef isn't going to be able to handle the 10 simultaneous users who will need to use your app.
So you want to check the performance of your app with multiple users, without actually exposing real people to a potentially slow app.

If you want to serve very large numbers of customers, benchmarking will also help you determine the maximum number of chefs you can fit in each kitchen (processes per server), and hence how many restaurants you need to build to serve a given number of users.

This is the process of benchmarking, first figuring out where your app is now, and then thinking about whether or not that's good enough.
The benchmarking process is supported by the [shinyloadtest](https://rstudio.github.io/shinyloadtest/) package and has three basic steps:

1.  Record a script that simulates a user interacting with your app.
    You can do this by running `shinyloadtest::record_session()` then interacting with your app like a user might.

2.  Replay the script with multiple simultaneous users with the shinycannon command-line app.

3.  Analyse the results using `shinyloadtest::load_runs()` and `shinyloadtest::report()`.

Here I'll give an overview of how each of the steps work; if you need more details, check out shinyloadtest's documentation and vignettes.

### Recording

If you're doing benchmarking on your laptop, you'll need to use two different R processes[^scaling-performance-5] --- one for Shiny, and one for shinyloadtest.

[^scaling-performance-5]: The easiest way to do this in RStudio, is just to open another RStudio instance.

-   In the first process, start your app and copy the url that it gives you:

    ```{r, eval = FALSE}
    runApp("myapp.R")
    #> Listening on http://127.0.0.1:7716
    ```

-   In the second process, paste the url into a `record_session()` call:

    ```{r, eval = FALSE}
    shinyloadtest::record_session("http://127.0.0.1:7716")
    ```

`record_session()` will open a new window containing a version of your app that records everything you do with it.
Now you need to interact with the app to simulate a "typical" user.
I recommend starting with a written script to guide your actions --- this will make it easier to repeat in the future, if you discover there's some important piece missing.
Your benchmarking will only be as good as your simulation, so you'll need to spend some time thinking about how to simulate a realistic interaction with the app.
For example, don't forget to add some pauses to reflect the thinking time that a real user would need.

Once you're done, close the app and shinyloadtest will save `recording.log` to your working directory.
This records every step you took in a way that can easily be replayed.
Keep a hold of it as you'll need it for the next step.

(While benchmarking works great on your laptop, you likely want to simulate the eventual deployment as closely as possible in order to get the most accurate results. So if your company has a special way of serving Shiny apps, talk to your IT folks about setting up a staging environment that you can use for load testing.)

### Replay

Now you have a script that represents the actions of a single user.
To simulate many people using your app, we're going to replay that script repeatedly using a special tool called shinycannon.
Unfortunately shinycannon is a bit of extra work to install because it's not an R package.
shinycannon is written in Java because the Java language is particularly well suited to the problem of performing tens or hundreds of web requests in parallel, using as few computational resources as possible.
This makes it possible for your laptop to both run the app and simulate many users.

Start by installing shinycannon by following the instructions at <https://rstudio.github.io/shinyloadtest/#shinycannon>

Then then shinycannon from the terminal like:

    shinycannon recording.log http://127.0.0.1:7716 \ 
      --workers 10 \
      --loaded-duration-minutes 5 \
      --output-dir run1

There are six arguments to `shinycannon`:

-   The first argument is a path to the recording that you created in the previous step.

-   The second argument is the url to your Shiny app (which you copied and pasted in the previous step).

-   `--workers` sets the number of parallel users to simulate.
    The above command will simulate the performance of your app as if 10 people were using it simultaneously.

-   `--loaded-duration-minutes` determines how long to run the test for.
    If this is longer than your script takes, shinycannon will just start the script again from the beginning.

-   `--output-dir` gives the name of the directory to save the output.
    You're likely to run the load test multiple times as you experiment with performance improvements, so strive to give these informative names.

When load testing for the first time, it's a good idea to start with a small number of workers and a short duration in order to quickly spot any major problems.

### Analysis

Now that you've simulated multi-user use of your app.
It's time to look at the results with

```{r, eval = FALSE}
library(shinyloadtest)
df <- load_runs(demo = "~/Downloads/scaling_case_study_run5/")
df
shinyloadtest_report(df, "report.html")
slt_session_duration(df)
```

Here I'll focus on the session duration plot, because that gives you the most insight into user experience.
To learn more about the other plots, you can read the [Analyzing Load Test Logs](https://rstudio.github.io/shinyloadtest/articles/analyzing-load-test-logs.html){.uri} article.

Red line shows the time that the original recording took.

If you're lucky, you might be able to stop here --- your app is fast enough for the number of users you expect.
In most cases, however, you'll need to do a little optimisation, which starts by finding where the bottleneck is.

## Profiling

If you want a chef to serve more customers, you need to do some time and motion studies to figure out what's slowing them down.
The equivalent in R is profiling, which basically regularly inspects the running code and records the call stack at each instant.

Note that it only records when the R is active; not when it's waiting (e.g. in `Sys.sleep()` or when downloading data over http), or when C code is being called.
This can be misleading, but does serve to concentrate your attention on what you can actually control within R.

<https://rstudio.github.io/profvis/examples.html>

What is call stack.
Section \@ref(reading-tracebacks).
Call stacks grow and shrink over time.
Show simple example.

```{r, eval = FALSE}
library(profvis)
profvis(runApp())
# perform the operation that's slow
# close the app
# look at the visualisation
```

Call stack diagram --- show code, then draw tree, then collapse into rectangles, then make width proportional to time.

Goal is to find the one slowest thing, because that has the highest payoff.
Once you've found it, brainstormed possible improvements and then tried them out, you look for the next slower thing.

Once you've isolated a slow part, if it's not already in a function, I highly recommend pulling it out as described in Chapter \@ref(scaling-functions).
Then make a minimal snippet of code that recreates the slowness you see in the app, so you can easily re-run it.
I also recommend writing a few tests, as in Chapter \@ref(scaling-testing), because in my experience the easiest way to make code faster is to make it incorrect 😆.

## Improve performance

Most techniques for speeding up your Shiny app are general; they're exactly what you'd do to speed up any R code.
If you want advice on how to speed up R code a couple of good places to start at the [Improving performance](https://adv-r.hadley.nz/perf-improve.html) of Advanced R and [Efficient R programming](https://csgillespie.github.io/efficientR/) by Colin Gillespie and Robin Lovelace.
I'm not going to repeat that advice here: instead, I'll focus on the topics that are most likely to affect your Shiny.

For more Shiny specific advice, I highly recommend watching Alan Dipert's rstudio::conf(2018) talk [Making Shiny fast by doing as little as possible](https://rstudio.com/resources/rstudioconf-2018/make-shiny-fast-by-doing-as-little-work-as-possible/){.uri}.

### Data import

First, make sure that any data is loaded outside of the server function, in the body of the `app.R`.
That ensures that the data is once per process, rather than once per user, which saves both time and memory.

Next, check that you're using the most efficient way to load your data:

-   If you have a flat file, try `data.table::fread()` or `vroom::vroom()` instead of `read.csv()` or `read.table()`.

-   If you have a data frame, saving with `arrow::write_feather()` and reading, try `arrow::read_feather()`.
    (<https://ursalabs.org/blog/2020-feather-v2/>)

-   Complex non-data frame, try `qs::qread()`/`qs::qsave()` instead of `readRDS()`/`saveRDS()`.

If that's still too slow, and each user only tends to use a small part of the full dataset, consider loading the data in a database.
Then you can easily retrieve only the data that the user specifically asks for.

### Data processing

After loading data from disk, it's common to do some basic cleaning and aggregation.
If this is expensive, you should consider using a cron job, scheduled RMarkdown report, or similar to perform the expensive operations and save the results.
This is like hiring a prep chef who comes in at 3am (when there are no customers) and does a bunch of work so that that chefs can be as efficient as possible.

### Share work across users

We discussed a specific type of caching for graphics in Section \@ref(cached-plots).
Shiny 1.6.0 introduces a general tool that works with any reactive: `withCache()`.
By default, reactives are already cached, but they only cache the previous value.
`withCache()` allows you to cache more values and to share those values across users.

To use the cache effectively, you'll need to have identified that a specific reactive is a bottleneck and done some thinking to make sure that the reactive is used multiple times or by multiple users.
(Also note that the impact of caching on your load tests is likely to be an over estimated because every simulated user does exactly the same thing, making it a perfect use case for caching).
Then:

`withCache()` is easy to use.
Just pipe the reactive into `withCache()`:

```{r, eval = FALSE}
r <- reactive(slow_function(input$x, input$y)) %>% 
  withCache(input$x, input$y)
```

The extra arguments to `withCache()` are the cache keys --- these are the values used to determine if a computation has occurred before and hence can be retrieved from the cache.

`withCache()` is usually paired with `withEvent()` because if a computation takes long enough that it's worth caching it, it's likely that you'll want to user to manually trigger with an action button or similar.

```{r, eval = FALSE}
r <- reactive() %>% 
  withCache(input$x, input$y) %>% 
  withEvent(input$go)
```

Like `renderCachedPlot()`, `withCache()` has a scope setting.
It defaults to `app` so that you get an in memory cache shared across all users of the app.
But you can `scope = "session"` so that each user session gets its own cache, or to `cachem::disk_cache()` to share across users, processes, and app restarts.
The more aggressively you cache, you more care you'll need to take to manually clear the cache when you change behaviour (e.g. the computation in a reactive) that's not captured by the cache key.

## Manage user expectations

As well as making your app faster, you can also make it seem faster.
There are three main useful techniques:

-   Require a button press before starting known slow tasks.

-   Notify the user when a slow operation starts, and when it ends, using the techniques of Section \@ref(notifications).

-   If possible, figure out how to draw progress bar, Section \@ref(progress-bars).
    There's good evidence that progress indicators make operations feel faster: <https://www.nngroup.com/articles/progress-indicators/>

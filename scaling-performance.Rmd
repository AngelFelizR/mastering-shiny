# Performance {#performance}

```{r, include = FALSE}
source("common.R")
options(tibble.print_min = 6, tibble.print_max = 6)
```

Shiny can support thousands or tens of thousands of users, if developed correctly.
But most Shiny apps were quickly thrown together to solve a pressing analytic need, and often start with pretty terrible performance.
In some ways, this is a feature of Shiny --- it allows you to quickly prototype a proof of concept that works for you, before figuring out how to make it fast so that it can be used by a bunch of people simultaneously.
But if you don't realise that app is likely because you've optimised for human iteration speed), making a Shiny app available to many people will be frustrating.

Fortunately, however, Shiny comes with a bunch of tools to help improve performance.
In this chapter you'll learn:

-   How to use shinyloadtest to simulate the use of your app by many people.
    This allows you to figure out if you have a problem, and to measure the impact of your performance improvement.

-   To use profvis to identify performance bottlenecks.
    It is extremely difficult to develop a good intuition for what parts of your app are likely to be slow.
    Fortunately there's no need because we can measure and visualise.

-   A grab bag of useful techniques to improve performance, particularly focussing on techniques that allow you to take advantage of multiple users.

-   We'll finish up with a little applied psychology to give a few techniques that can help your app *feel* as fast as possible.

For a demo of the whole process, I recommend Joe Cheng's rstudio::conf(2019) keynote [Shiny in production: principles, best practices, and tools](https://rstudio.com/resources/rstudioconf-2019/shiny-in-production-principles-practices-and-tools/){.uri} where he works through the whole process with a realistic app.
This is also written in the [scaling an app case study](https://rstudio.github.io/shinyloadtest/articles/case-study-scaling.html) on the shinyloadtest website.

But to get started, we'll build up your mental model of Shiny performance with a restaurant metaphor.
We'll use this metaphor throughout the rest of the chapter.

```{r setup}
library(shiny)
```

Particularly thanks go to my RStudio colleagues Joe Cheng, Sean Lopp, and Alan Dipert, whose RStudio::conf() talks were particularly helpful when writing this chapter.

## Dining at restaurant Shiny

When considering performance, it's useful to think of a Shiny app as a restaurant[^scaling-performance-1].
Each customer (user) comes into the restaurant (the server) and makes an order (a request), which is then prepared by a chef (the R process).
This metaphor is useful because like a restaurant, one R process can serve multiple users at the same time, and there similar ways to dealing with increased demand.

[^scaling-performance-1]: Thanks to Sean Lopp for this analogy from his rstudio::conf(2018) talk [Scaling Shiny to 10,000 users](https://rstudio.com/resources/rstudioconf-2018/scaling-shiny/){.uri}.
    I highly recommend watching it if you have any doubt that Shiny apps can handle thousands of users.

To begin, you might investigate ways to make your current chef more efficient (optimise your R code).
To do so, you'd first spend some time watching to find the bottlenecks in their method (profiling) and then brainstorming ways to make it faster (optimising).
For example, maybe you hire a prep cook who can come in before the first customer and chop some vegetables (prepares the data), or you could invest in a time-saving gadget (a faster R package).

Or you might think about adding more chefs (processes) to the kitchen (server).
Fortunately, it's much easier[^scaling-performance-2] to add more processes than hire trained chefs.
If you keep hiring more chefs, eventually the kitchen will get too full and you'll need to add more equipment.
This is called scaling **up**[^scaling-performance-3], adding more resources (memory or cores) to an existing server to allow it to run more processes
.

[^scaling-performance-2]: Again, this depends on exactly how your app is deployed, but it's typically a prominently labelled setting.

[^scaling-performance-3]: Or vertical scaling

At some point, you'll have crammed as many chefs into your restaurant as you possibly can and its still not enough.
At that point, you'll need to building more restaurants.
This is called scaling **out**[^scaling-performance-4], and means using multiple servers.
This allows you to scale to any number of customers, but there's no way to serve one customer from multiple restaurants, so you need some way to direct customers to the least busy restaurant (a load balancer).
I won't talk more about scaling out in this chapter, because while the details are straightforward, they depend entirely on your deployment infrastructure.
It's good to know this option exists, and allows you app to scale to any number of users.

[^scaling-performance-4]: Or horizontal scaling

There's one major place where the metaphor breaks down: a normal chef can make multiple dishes at the same time, carefully interweaving the steps to do as much as possible in parallel.
Unfortunately, R can't do multiple things at the same because it's single threaded, i.e. each process can only do one thing at a time.
This is fine if all of the meals are fast to cook, but if someone requests 24-hour sous vide pork belly, everyone else has to wait until it's done.
Fortunately, you can work around this limitation using async programming.
Unfortunately, that's a complex topic and beyond the scope of this book, but you can learn more at <https://rstudio.github.io/promises/>.

## Benchmark

You almost always start by developing an app for yourself.
You have a personal chef who only ever has to serve one customer at a time (you!).
While you might be happy with performance, you might worry that your chef isn't going to be able to handle the 10 simultaneous users who will need to use your app.
So you want to check the performance of your app with multiple users, without actually exposing real people to a potentially slow app.

If you want to serve very large numbers of customers, benchmarking will also help you determine the maximum number of chefs you can fit in each kitchen (processes per server), and hence how many restaurants you need to build to serve a given number of users.

This is the process of benchmarking, first figuring out where your app is now, and then thinking about whether or not that's good enough.
The benchmarking process is supported by the [shinyloadtest](https://rstudio.github.io/shinyloadtest/) package and has three basic steps:

1.  Record a script that simulates a user interacting with your app.
    You can do this by running `shinyloadtest::record_session()` then interacting with your app like a user might.

2.  Replay the script with multiple simultaneous users with the shinycannon command-line app.

3.  Analyse the results using `shinyloadtest::load_runs()` and `shinyloadtest::report()`.

Here I'll give an overview of how each of the steps work; if you need more details, check out shinyloadtest's documentation and vignettes.

### Recording

If you're doing benchmarking on your laptop, you'll need to use two different R processes[^scaling-performance-5] --- one for Shiny, and one for shinyloadtest.

[^scaling-performance-5]: The easiest way to do this in RStudio, is just to open another RStudio instance.

-   In the first process, start your app and copy the url that it gives you:

    ```{r, eval = FALSE}
    runApp("myapp.R")
    #> Listening on http://127.0.0.1:7716
    ```

-   In the second process, paste the url into a `record_session()` call:

    ```{r, eval = FALSE}
    shinyloadtest::record_session("http://127.0.0.1:7716")
    ```

`record_session()` will open a new window containing a version of your app that records everything you do with it.
Now you need to interact with the app to simulate a "typical" user.
I recommend starting with a written script to guide your actions --- this will make it easier to repeat in the future, if you discover there's some important piece missing.
Your benchmarking will only be as good as your simulation, so you'll need to spend some time thinking about how to simulate a realistic interaction with the app.
For example, don't forget to add some pauses to reflect the thinking time that a real user would need.

Once you're done, close the app and shinyloadtest will save `recording.log` to your working directory.
This records every step you took in a way that can easily be replayed.
Keep a hold of it as you'll need it for the next step.

(While benchmarking works great on your laptop, you likely want to simulate the eventual deployment as closely as possible in order to get the most accurate results. So if your company has a special way of serving Shiny apps, talk to your IT folks about setting up a staging environment that you can use for load testing.)

### Replay

Now you have a script that represents the actions of a single user.
To simulate many people using your app, we're going to replay that script repeatedly using a special tool called shinycannon.
Unfortunately shinycannon is a bit of extra work to install because it's not an R package.
shinycannon is written in Java because the Java language is particularly well suited to the problem of performing tens or hundreds of web requests in parallel, using as few computational resources as possible.
This makes it possible for your laptop to both run the app and simulate many users.

Start by installing shinycannon by following the instructions at <https://rstudio.github.io/shinyloadtest/#shinycannon>

Then then shinycannon from the terminal like:

    shinycannon recording.log http://127.0.0.1:7911 \
      --workers 10 \
      --loaded-duration-minutes 5 \
      --output-dir run1

There are six arguments to `shinycannon`:

-   The first argument is a path to the recording that you created in the previous step.

-   The second argument is the url to your Shiny app (which you copied and pasted in the previous step).

-   `--workers` sets the number of parallel users to simulate.
    The above command will simulate the performance of your app as if 10 people were using it simultaneously.

-   `--loaded-duration-minutes` determines how long to run the test for.
    If this is longer than your script takes, shinycannon will just start the script again from the beginning.

-   `--output-dir` gives the name of the directory to save the output.
    You're likely to run the load test multiple times as you experiment with performance improvements, so strive to give these informative names.

When load testing for the first time, it's a good idea to start with a small number of workers and a short duration in order to quickly spot any major problems.

### Analysis

Now that you've simulated multiple people using your app simultaneously, it's time to look at the results.
First, load the data into R using `load_runs()`:

```{r, eval = FALSE}
library(shinyloadtest)
df <- load_runs("scaling-testing/run1")
```

```{r, echo = FALSE}
library(shinyloadtest)
if (file.exists("scaling-testing.rds")) {
  df <- readRDS("scaling-testing.rds")
} else {
  df <- load_runs("scaling-testing/run1")  
  saveRDS(df, "scaling-testing.rds")
}
```

This produces a tidy tibble that you can analyse by hand, if you want.
But typically you'll run the standard shinyloadtest report which produces a standalone HTML file containing the most important results.

```{r, eval = FALSE}
shinyloadtest_report(df, "report.html")
```

I'm not going to discuss all the pages here, and instead I'll just focus on what I think is the most important plot: the session duration.
To learn more about the other pages, I highly recommend reading the [Analyzing Load Test Logs](https://rstudio.github.io/shinyloadtest/articles/analyzing-load-test-logs.html){.uri} article.

```{r, echo = FALSE}
slt_session_duration(df)
```

The **session duration** plot displays each simulated user session as row.
Each event is a rectangle with width proportion to time taken, and the colour is given by the event type.
The red line shows the time that the original recording took; a useful reference.

-   Does the app perform equivalently under load as it does for a single user?
    If so, you're done.
    Your app is fast enough for your use case.

-   Is the slowness in homepage, css/js, or start session?
    If so, you'll need to:

-   Otherwise, and most typically, the slowness will be in the calculation, and you'll need to use the techniques in the following two sections to find and fix slow operations.

## Profiling

Once you've confirmed that your restaurant can't perform under the number of customers you expect, you're going to need to do some research to figure out what's wrong.
In a restaurant, you might do a time and motion study, following your chefs around to figure where they're spending their time, and how you might be able to help them to be more efficient.
For a Shiny app, you're going to perform **profiling**: tracking down how long each function call takes so that you can find the slowest and make it faster.

To perform profiling, we're going to turn the [profvis](https://rstudio.github.io/profvis) package, which provides an interactive visualisation of the profiling data collected by R's `RProf()` function.
To understand what profiling does, you need to understand a little bit about the call stack.
We discussed calls stacks previously in Section \@ref(reading-tracebacks) in the context of debugging.

### Flame graphs

The most common form of profiling visualisation is the flame graph, and is used by profvis so.
I'll start by explaining it, step-by-step.
To make it concrete, take the following code:

```{r}
f <- function() {
  pause(0.2)
  g()
  h()
}
g <- function() {
  pause(0.1)
  h()
}
h <- function() {
  pause(0.3)
}
```

If I asked to to mentally run this code, and record the call stacks at each step you might right something like this:

-   f

-   f \> g

-   f \> g \> h

-   f \> h

Which we could draw like:

```{r echo = FALSE, out.width = NULL, fig.align = "center"}
knitr::include_graphics("diagrams/scaling-performance/vertical.png", dpi = 300)
```

I think it's easiest to understand when you think about time flowing from top-to-bottom, but by convention these diagrams are usually drawn from left to right, so we rotate it 90 degrees:

```{r echo = FALSE, out.width = NULL, fig.align = "center"}
knitr::include_graphics("diagrams/scaling-performance/horizontal.png", dpi = 300)
```

We can make this diagram more informative by making the width of each call proportional to the amount of time it takes:

```{r echo = FALSE, out.width = NULL, fig.align = "center"}
knitr::include_graphics("diagrams/scaling-performance/proportional.png", dpi = 300)
```

And make this a little clearer by collapsing adjacent calls:

```{r echo = FALSE, out.width = NULL, fig.align = "center"}
knitr::include_graphics("diagrams/scaling-performance/collapsed.png", dpi = 300)
```

This is a flame graph!
You might wonder why it's a called a flame graph.
That's answered by applying the most common colour scheme found in the wild:

```{r echo = FALSE, out.width = NULL, fig.align = "center"}
knitr::include_graphics("diagrams/scaling-performance/flame.png", dpi = 300)
```

However, since the colours are random "warm" colours (meant evoke the computer running "hot") we usually omit them and stick with black and white.
You can learn more about the history of flame graph in <https://queue.acm.org/detail.cfm?id=2927301>.

### profvis

Running profvis itself is easy.
Just wrap the code you want to profile in `profvis::profvis()`:

```{r, eval = FALSE}
profvis::profvis(f())
```

That gives a result that's very similar to graphs that I drew by hand:

```{r echo = FALSE, out.width = NULL, fig.align = "center"}
knitr::include_graphics("images/scaling-performance/profvis.png", dpi = 300)
```

There are a couple of important differences to note:

-   R's profiler works by stopping execution every 10ms and recording the call stack.
    It's not possible to stop exactly every 10ms (R might be in the middle of something that can't be interrupted) so the results are subject to a small amount of random variation.

-   The profile can only record time when R code is running.
    It can't currently capture time in various special calls the R has no insight into (e.g. Sys.sleep, reading from the internet).
    This can be misleading, but does serve to concentrate your attention on what you can actually control within R.

### Using profiling data

Works similarly for Shiny apps:

```{r, eval = FALSE}
profvis::profvis(runApp("path/to/app"))
```

And there's a case study of a Shiny app at <https://rstudio.github.io/profvis/examples.html>.

Goal is to find the one slowest thing, because that has the highest payoff.
Once you've isolated a slow part, if it's not already in a function, I highly recommend pulling it out as described in Chapter \@ref(scaling-functions).
Then make a minimal snippet of code that recreates the slowness you see in the app, so you can easily re-run it.
You'll need to do that many times as you try out possible improvements as suggested in the next section.

I also recommend writing a few tests, as in Chapter \@ref(scaling-testing), because in my experience the easiest way to make code faster is to make it incorrect 😆.

## Improve performance

Most techniques for speeding up your Shiny app are general; they're exactly what you'd do to speed up any R code.
If you want advice on how to speed up R code a couple of good places to start at the [Improving performance](https://adv-r.hadley.nz/perf-improve.html) of Advanced R and [Efficient R programming](https://csgillespie.github.io/efficientR/) by Colin Gillespie and Robin Lovelace.
I'm not going to repeat that advice here: instead, I'll focus on the topics that are most likely to affect your Shiny.

For more Shiny specific advice, I highly recommend watching Alan Dipert's rstudio::conf(2018) talk [Making Shiny fast by doing as little as possible](https://rstudio.com/resources/rstudioconf-2018/make-shiny-fast-by-doing-as-little-work-as-possible/){.uri}.

### Data import

First, make sure that any data is loaded outside of the server function, in the body of the `app.R`.
That ensures that the data is once per process, rather than once per user, which saves both time and memory.

Next, check that you're using the most efficient way to load your data:

-   If you have a flat file, try `data.table::fread()` or `vroom::vroom()` instead of `read.csv()` or `read.table()`.

-   If you have a data frame, saving with `arrow::write_feather()` and reading, try `arrow::read_feather()`.
    (<https://ursalabs.org/blog/2020-feather-v2/>)

-   Complex non-data frame, try `qs::qread()`/`qs::qsave()` instead of `readRDS()`/`saveRDS()`.

If that's still too slow, and each user only tends to use a small part of the full dataset, consider loading the data in a database.
Then you can easily retrieve only the data that the user specifically asks for.

### Data processing

After loading data from disk, it's common to do some basic cleaning and aggregation.
If this is expensive, you should consider using a cron job, scheduled RMarkdown report, or similar to perform the expensive operations and save the results.
This is like hiring a prep chef who comes in at 3am (when there are no customers) and does a bunch of work so that that chefs can be as efficient as possible.

### Share work across users

We discussed a specific type of caching for graphics in Section \@ref(cached-plots).
Shiny 1.6.0 introduces a general tool that works with any reactive: `withCache()`.
By default, reactives are already cached, but they only cache the previous value.
`withCache()` allows you to cache more values and to share those values across users.

To use the cache effectively, you'll need to have identified that a specific reactive is a bottleneck and done some thinking to make sure that the reactive is used multiple times or by multiple users.
(Also note that the impact of caching on your load tests is likely to be an over estimated because every simulated user does exactly the same thing, making it a perfect use case for caching).
Then:

`withCache()` is easy to use.
Just pipe the reactive into `withCache()`:

```{r, eval = FALSE}
r <- reactive(slow_function(input$x, input$y)) %>% 
  withCache(input$x, input$y)
```

The extra arguments to `withCache()` are the cache keys --- these are the values used to determine if a computation has occurred before and hence can be retrieved from the cache.

`withCache()` is usually paired with `withEvent()` because if a computation takes long enough that it's worth caching it, it's likely that you'll want to user to manually trigger with an action button or similar.

```{r, eval = FALSE}
r <- reactive() %>% 
  withCache(input$x, input$y) %>% 
  withEvent(input$go)
```

Like `renderCachedPlot()`, `withCache()` has a scope setting.
It defaults to `app` so that you get an in memory cache shared across all users of the app.
But you can `scope = "session"` so that each user session gets its own cache, or to `cachem::disk_cache()` to share across users, processes, and app restarts.
The more aggressively you cache, you more care you'll need to take to manually clear the cache when you change behaviour (e.g. the computation in a reactive) that's not captured by the cache key.

## Manage user expectations

Finally, there are a few design considerations worth considering when you know that parts of your app are slow.
These don't make your app any faster, but they can make it feel faster, and they can certainly improve the overall user experience of your app.

-   Split your app up into tabs, using `tabsetPanel()`.
    Only outputs on the current tab are recomputed, so you can use this to focus computation on what the user is currently looking at.

-   Use a button press to initiate a slow operation.
    An action button combined with `eventReactive()` allows the user to explicitly request a slow operation, which is less frustrating.
    Once the operation starts, make sure to use the notification techniques from Section \@ref(notifications) to let the user know what's happen.
    If possible, display an incremental progress bar, Section \@ref(progress-bars), because there's good evidence that progress bars make operations feel faster: <https://www.nngroup.com/articles/progress-indicators/>.

-   If the app requires significant work to happen on startup (and you can't reduce it with preprocessing), make sure to design your app so that the HTML appears, and you can let the user know that they'll need to wait.

-   Finally, if you want to keep the app responsive while some expensive operation happens in the background, it's time to learn about async programming: [https://rstudio.github.io/promises](https://rstudio.github.io/promises/index.html).

## Summary

This chapter has given you the tools to precise measure and improve the performance of any shiny app.
You learned about shinyloadtest to measure the performance, using shinycannon to simulate multiple users working with your app the at same time.
Then you learned how to use profvis to find the single most expensive operation, and a grab-bag of techniques that you can use to improve it.

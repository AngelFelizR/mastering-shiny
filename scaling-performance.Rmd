# Performance {#performance}

```{r, include = FALSE}
source("common.R")
options(tibble.print_min = 6, tibble.print_max = 6)
```

Shiny can support thousands or tens of thousands of users, if developed correctly.
But most Shiny apps were quickly thrown together to solve a pressing analytic need, and often start with pretty terrible performance.
In some ways, this is a feature of Shiny --- it allows you to quickly prototype a proof of concept that works for you, before figuring out how to make it fast so that it can be used by a bunch of people simultaneously.
But if you don't realise that app is likely because you've optimised for human iteration speed), making a Shiny app available to many people will be frustrating.

Fortunately, however, Shiny comes with a bunch of tools to help improve performance.
In this chapter you'll learn:

-   How to use shinyloadtest to simulate the use of your app by many people.
    This allows you to figure out if you have a problem, and to measure the impact of your performance improvement.

-   To use profvis to identify performance bottlenecks.
    It is extremely difficult to develop a good intuition for what parts of your app are likely to be slow.
    Fortunately there's no need because we can measure and visualise.

-   A grab bag of useful techniques to improve performance, particularly focussing on techniques that allow you to take advantage of multiple users.

-   We'll finish up with a little applied psychology to give a few techniques that can help your app *feel* as fast as possible.

But to get started, we'll build up your mental model of Shiny performance with a restaurant metaphor.
We'll use this metaphor throughout the rest of the chapter.

```{r setup}
library(shiny)
```

Particularly thanks go to my RStudio colleagues Joe Cheng, Sean Lopp, and Alan Dipert, whose RStudio::conf() talks were particularly helpful when writing this chapter.

## Dining at restaurant Shiny

When considering performance, it's useful to think of a Shiny app as a restaurant[^scaling-performance-1].
Each customer (user) comes into the restaurant (the server) and makes an order (a request), which is then prepared by a chef (the R process).
This metaphor is useful because like a restaurant, one R process can serve multiple users at the same time, and there similar ways to dealing with increased demand.

[^scaling-performance-1]: Thanks to Sean Lopp for this analogy from his rstudio::conf(2018) talk [Scaling Shiny to 10,000 users](https://rstudio.com/resources/rstudioconf-2018/scaling-shiny/){.uri}.
    I highly recommend watching it if you have any doubt that Shiny apps can handle thousands of users.

To begin, you might investigate ways to make your current chef more efficient (optimise your R code).
To do so, you'd first spend some time watching to find the bottlenecks in their method (profiling) and then brainstorming ways to make it faster (optimising).
For example, maybe you hire a prep cook who can come in before the first customer and chop some vegetables (prepares the data), or you could invest in a time-saving gadget (a faster R package).

Or you might think about adding more chefs (processes) to the kitchen (server).
Fortunately, it's much easier[^scaling-performance-2] to add more processes than hire trained chefs.
If you keep hiring more chefs, eventually the kitchen will get too full and you'll need to add more equipment.
This is called scaling **up**[^scaling-performance-3], adding more resources (memory or cores) to an existing server to allow it to run more processes
.

[^scaling-performance-2]: Again, this depends on exactly how your app is deployed, but it's typically a prominently labelled setting.

[^scaling-performance-3]: Or vertical scaling

At some point, you'll have crammed as many chefs into your restaurant as you possibly can and its still not enough.
At that point, you'll need to building more restaurants.
This is called scaling **out**[^scaling-performance-4], and means using multiple servers.
This allows you to scale to any number of customers, but there's no way to serve one customer from multiple restaurants, so you need some way to direct customers to the least busy restaurant (a load balancer).
I won't talk more about scaling out in this chapter, because while the details are straightforward, they depend entirely on your deployment infrastructure.
It's good to know this option exists, and allows you app to scale to any number of users.

[^scaling-performance-4]: Or horizontal scaling

There's one major place where the metaphor breaks down: a normal chef can make multiple dishes at the same time, carefully interweaving the steps to do as much as possible in parallel.
Unfortunately, R can't do multiple things at the same because it's single threaded, i.e. each process can only do one thing at a time.
This is fine if all of the meals are fast to cook, but if someone requests 24-hour sous vide pork belly, everyone else has to wait until it's done.
Fortunately, you can work around this limitation using async programming.
Unfortunately, that's a complex topic and beyond the scope of this book, but you can learn more at <https://rstudio.github.io/promises/>.

## Benchmark

The first step is always to identify if you have a problem.
Currently your restaurant only servers one customer (you), and while you're happy with performance you worry what will happen if 20 people come in for lunch at the same time.

How can you tell if your app is going to be slow for multiple users without deploying it and have a bunch of your friends try it out at the same time?

The is the problem that the [shinyloadtest](https://rstudio.github.io/shinyloadtest/) package is designed to solve.
It has four basic steps:

1.  Run your app locally and use `shinyloadtest::record_session()` to record an prototypical user session.
2.  Replay the script with multiple users using shinycannon (a command line tool included with shinyload test)
3.  Analyse the results using `shinyloadtest::load_runs()` and `shinyloadtest::report()`.

For a demo of the whole process from benchmarking to profiling to improvement and back, I recommend Joe Cheng's rstudio::conf(2019) keynote [Shiny in production: principles, best practices, and tools](https://rstudio.com/resources/rstudioconf-2019/shiny-in-production-principles-practices-and-tools/){.uri} where he works through the whole process with a realistic app.
This is also written in the [scaling an app case study](https://rstudio.github.io/shinyloadtest/articles/case-study-scaling.html) on the shinyloadtest website.

### Recording

Start your app then run `shinyloadtest::record_session`.
If you're doing this all locally you'll need to use two different R session --- one for Shiny, and one for shinyloadtest.
You'll need to copy and paste the url that `runApp()` gives you:

```{r, eval = FALSE}
runApp("myapp.R")
#> Listening on http://127.0.0.1:7716
```

Then in another R session, run `shinyloadtest::record_session(http://127.0.0.1:7716)`.

I recommend making a written script for yourself to guide your actions.
Your measurements will only be as good as your script.
Try to capture all of the most important actions in a realistic way, adding pauses to reflect the thinking time that a real user would take

Note that while you can do everything on your own laptop, you're best off simulating the actual deployment as closely as possible.
For example, if you're going to deploy the app publicly using Shiny Server pro, make sure to replay the user session against your live app.
And the server that you record on must match the server you replay on.

### Replay

To replay your script you need a separate command line tool called shinycannon.
Unfortunately this is a bit more work to install because it's written in Java rather than R.
Java is particularly well suited to this problem of running tens or hundreds of web requests in parallel, using as few computational resources as possible (so it's possible for your laptop to both run the app and simulate many users if needed).

So start by following the instructions at <https://rstudio.github.io/shinyloadtest/#shinycannon>

Then you'll run shinycannon from the terminal like:

    shinycannon recording.log {app_url} --workers 10 --loaded-duration-minutes 5 --output-dir run1

You'll need to choose how many workers to run (based on how many people you expect will use your app simultaneously) and how long to run for.
If the run time is longer than your script, shinycannon will just re-run it from the beginning.
If you're load testing for the first time, you should pick a small number of workers and short time, as it's likely the problems will be obvious.

### Analysis

```{r, eval = FALSE}
library(shinyloadtest)
df <- load_runs(demo = "~/Downloads/scaling_case_study_run5/")
slt_session_duration(df)
```

Full details in <https://rstudio.github.io/shinyloadtest/articles/analyzing-load-test-logs.html>

Red line shows the time that the original recording took.
If you are thinking about scaling horizontally, you will want to increase the number of workers until you see this be far away.
this process will also help you to understand how many how many customers each chef can serve (how many users per R process) and how many chefs can fit in a kitchen (how many processes per server), and hence how many kitchens you need to build.

## Profiling

If you want a chef to serve more customers, you need to do some time and motion studies to figure out what's slowing them down.
The equivalent in R is profiling, which basically regularly inspects the running code and records the call stack at each instant.

Note that it only records when the R is active; not when it's waiting (e.g. in `Sys.sleep()` or when downloading data over http), or when C code is being called.
This can be misleading, but does serve to concentrate your attention on what you can actually control within R.

What is call stack.
Section \@ref(reading-tracebacks).
Call stacks grow and shrink over time.

```{r, eval = FALSE}
library(profvis)
profvis(runApp())
# perform the operation that's slow
# close the app
# look at the visualisation
```

Call stack diagram --- show code, then draw tree, then collapse into rectangles, then make width proportional to time.

Goal is to find the one slowest thing, because that has the highest payoff.
Once you've found it, brainstormed possible improvements and then tried them out, you look for the next slower thing.

Once you've isolated a slow part, if it's not already in a function, I highly recommend pulling it out as in Chapter \@ref(scaling-functions).
That will make it much easier to optimise.
Also recommend testing it, because it at least in my experience the easiest way to make code faster is to make it incorrect 😆.

## Do less work

Most techniques are general --- follow advice in Advanced R. But there's some particular techniques unique to Shiny because of the multiple users.
Often you can save time by sharing work across users.
Don't repeat yourself.

For more, I highly recommend watching Alan Dipert's rstudio::conf(2018) talk [Making Shiny fast by doing as little as possible](https://rstudio.com/resources/rstudioconf-2018/make-shiny-fast-by-doing-as-little-work-as-possible/){.uri}.

### Data import

First, make sure that any common data is loaded outside of the server function, in the body of the `app.R`.
That ensures that the data is once per process, rather than once per user, which saves both time and memory.

Next, check that you're using the most efficient way to load your data:

-   If you have a flat file, try `data.table::fread()` or `vroom::vroom()` instead of `read.csv()` or `read.table()`.

-   If you have a data frame, saving with `arrow::write_feather()` and reading, try `arrow::read_feather()`.
    (<https://ursalabs.org/blog/2020-feather-v2/>)

-   Complex non-data frame, try `qs::qread()`/`qs::qsave()` instead of `readRDS()`/`saveRDS()`.

If that's still too slow, and each user only tends to use a small part of the full dataset, consider loading the data in a database.
Then you can easily retrieve only the data that the user specifically asks for.

### Data processing

After loading data from disk, it's common to do some basic cleaning and aggregation.
If this is expensive, you should consider using cron job (or scheduled RMarkdown reports) or similar to save the precomputed results.
To continue the restaurant analogy --- this is like hiring a prep chef who comes in at 3am (when there are no customers) and does a bunch of work so that that chefs can be as efficient as possible.

### Share work across users

We discussed a specific type of caching for graphics in Section \@ref(cached-plots).
Shiny 1.6.0 introduces a general tool that works with any reactive: `withCache()`.
By default, reactives are already cached, but they only cache the previous value.
`withCache()` allows you to cache more values and to share those values across users.

To use the cache effectively, you'll need to have identified that a specific reactive is a bottleneck and done some thinking to make sure that the reactive is used multiple times or by multiple users.
(Also note that the impact of caching on your load tests is likely to be an over estimated because every simulated user does exactly the same thing, making it a perfect use case for caching).
Then:

`withCache()` is easy to use.
Just pipe the reactive into `withCache()`:

```{r, eval = FALSE}
r <- reactive(slow_function(input$x, input$y)) %>% 
  withCache(input$x, input$y)
```

The extra arguments to `withCache()` are the cache keys --- these are the values used to determine if a computation has occurred before and hence can be retrieved from the cache.

`withCache()` is usually paired with `withEvent()` because if a computation takes long enough that it's worth caching it, it's likely that you'll want to user to manually trigger with an action button or similar.

```{r, eval = FALSE}
r <- reactive() %>% 
  withCache(input$x, input$y) %>% 
  withEvent(input$go)
```

Like `renderCachedPlot()`, `withCache()` has a scope setting.
It defaults to `app` so that you get an in memory cache shared across all users of the app.
But you can `scope = "session"` so that each user session gets its own cache, or to `cachem::disk_cache()` to share across users, processes, and app restarts.
The more aggressively you cache, you more care you'll need to take to manually clear the cache when you change behaviour (e.g. the computation in a reactive) that's not captured by the cache key.

## Manage user expectations

As well as making your app faster, you can also make it seem faster.

<https://www.nngroup.com/articles/progress-indicators/>

Require confirmation before known slow interaction.
Show a Progress bar.
Techniques of Chapter \@ref(action-feedback)

```{r, eval = FALSE}
r <- reactive({
  id <- showNotification("Reading data...", duration = NULL, closeButton = FALSE)
  on.exit(removeNotification(id), add = TRUE)
  
  read.csv(input$path)
}) %>% 
  withCache(input$x, input$y) %>% 
  withEvent(input$go)
```

